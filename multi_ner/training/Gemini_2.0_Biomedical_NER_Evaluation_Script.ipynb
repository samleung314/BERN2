{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add entity type to BIO tagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to ./NERdata_gemini/NCBI-disease/test_disease.txt\n"
     ]
    }
   ],
   "source": [
    "def process_bio_file(input_file, output_file, entity_type):\n",
    "    \"\"\"\n",
    "    Process a BIO-tagged text file and add a third column only when the tag is B or I.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input file\n",
    "        output_file (str): Path to the output file\n",
    "        entity_type (str): Type of entity\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "\n",
    "    processed_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:  # Keep empty lines as is\n",
    "            processed_lines.append('')\n",
    "            continue\n",
    "\n",
    "        parts = line.split()\n",
    "        if len(parts) != 2:\n",
    "            # Keep lines that don't match the expected format\n",
    "            processed_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        word, tag = parts\n",
    "\n",
    "        if tag in ['B', 'I']:\n",
    "            # Add a third column for B or I tags\n",
    "            processed_line = f\"{word} {tag} {entity_type}\"\n",
    "        else:\n",
    "            # Keep the original two columns for other tags\n",
    "            processed_line = f\"{word} {tag}\"\n",
    "\n",
    "        processed_lines.append(processed_line)\n",
    "\n",
    "    # Write the processed lines to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write('\\n'.join(processed_lines))\n",
    "\n",
    "    print(f\"Processed file saved to {output_file}\")\n",
    "\n",
    "\n",
    "input_file = \"./NERdata_gemini/NCBI-disease/test.txt\"\n",
    "output_file = \"./NERdata_gemini/NCBI-disease/test_disease.txt\"\n",
    "entity_type =  \"disease\"\n",
    "process_bio_file(input_file, output_file, entity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Configuration for Gemini API\n",
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "\n",
    "# Initialize the Google Generative AI SDK\n",
    "# client = genai.Client(vertexai=True, api_key=API_KEY)\n",
    "#                       # http_options=types.HttpOptions(api_version='v1'))\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project='vertexai-375019',\n",
    "    location='us-east4',\n",
    "    # http_options=types.HttpOptions(api_version='v1')\n",
    ")\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Evaluate Gemini 2.0-flash on biomedical NER tasks')\n",
    "    parser.add_argument('--data_dir', type=str, required=True,\n",
    "                        help='Directory containing the evaluation data')\n",
    "    parser.add_argument('--output_dir', type=str, default='results',\n",
    "                        help='Directory to save the evaluation results')\n",
    "    parser.add_argument('--system_prompt', type=str, required=True,\n",
    "                        help='Path to the system prompt file for Gemini')\n",
    "    parser.add_argument('--batch_size', type=int, default=1,\n",
    "                        help='Number of examples to process in each batch')\n",
    "    parser.add_argument('--max_seq_length', type=int, default=512,\n",
    "                        help='Maximum sequence length for processing')\n",
    "    # parser.add_argument('--save_intermediate', action='store_true',\n",
    "    #                     help='Save intermediate entity extraction results')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bio_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Read BIO-tagged data file and convert to a list of examples.\n",
    "    Each example contains tokens and their gold labels.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    current_entity_types = []\n",
    "\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Empty line indicates the end of a sentence\n",
    "            if not line:\n",
    "                if current_tokens:\n",
    "                    examples.append({\n",
    "                        'tokens': current_tokens,\n",
    "                        'labels': current_labels,\n",
    "                        'text': ' '.join(current_tokens),\n",
    "                        'entity_types' : current_entity_types\n",
    "                    })\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "                    current_entity_types = []\n",
    "                continue\n",
    "\n",
    "            # Split line into token and BIO tag\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                token, label = parts[0], parts[1]\n",
    "                current_tokens.append(token)\n",
    "                current_labels.append(label)\n",
    "                if len(parts) == 3:\n",
    "                  current_entity_types.append(parts[2])\n",
    "                else:\n",
    "                  current_entity_types.append('')\n",
    "    # Add the last example if it exists\n",
    "    if current_tokens:\n",
    "        examples.append({\n",
    "            'tokens': current_tokens,\n",
    "            'labels': current_labels,\n",
    "            'text': ' '.join(current_tokens),\n",
    "            'entity_types' : current_entity_types\n",
    "        })\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bio_to_entities(tokens: List[str], labels: List[str], entity_types: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert BIO labels to entity spans.\n",
    "    Returns a list of entity dictionaries with begin, end, mention, and type.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    i = 0\n",
    "    start_pos = 0\n",
    "    end_pos = 0\n",
    "    while i < len(labels):\n",
    "        if labels[i].startswith('B'):\n",
    "            entity_type = entity_types[i]\n",
    "            start = i\n",
    "            end = i\n",
    "            end_pos += len(tokens[i])\n",
    "\n",
    "            # Find the end of this entity (all consecutive I- tags)\n",
    "            i += 1\n",
    "            while i < len(labels) and labels[i].startswith('I'):\n",
    "                end = i\n",
    "                end_pos += len(tokens[i])\n",
    "                i += 1\n",
    "\n",
    "            # Extract the entity text and add to entities list\n",
    "            entity_text = ' '.join(tokens[start:end+1])\n",
    "            entities.append({\n",
    "                'begin': start_pos,\n",
    "                'end': end_pos,\n",
    "                'mention': entity_text,\n",
    "                'type': entity_type\n",
    "            })\n",
    "            start_pos = end_pos\n",
    "        else:\n",
    "            start_pos += len(tokens[i])\n",
    "            end_pos = start_pos\n",
    "            i += 1\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_api(prompt: str, system_prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a request to the Gemini API using Google Generative AI SDK and return the response.\n",
    "    Reads the system prompt from a file.\n",
    "    \"\"\"\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"GEMINI_API_KEY environment variable not set\")\n",
    "\n",
    "    try:\n",
    "        # Generate content\n",
    "        response = client.models.generate_content(\n",
    "          model='gemini-2.0-flash',\n",
    "          contents=types.Part.from_text(text=prompt),\n",
    "          config=types.GenerateContentConfig(\n",
    "              system_instruction=system_prompt,\n",
    "              temperature=0,\n",
    "              top_p=0.95,\n",
    "              top_k=20,\n",
    "              candidate_count=1,\n",
    "              seed=0,\n",
    "              max_output_tokens=8192,\n",
    "              stop_sequences=['STOP!'],\n",
    "              presence_penalty=0.0,\n",
    "              frequency_penalty=0.0,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Convert response to dictionary format for compatibility with the rest of the code\n",
    "        return {\n",
    "            \"candidates\": [\n",
    "                {\n",
    "                    \"content\": {\n",
    "                        \"parts\": [{\"text\": response.text}]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini API: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_response(response: Dict[str, Any], text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract entities from the Gemini API response with the new format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the response text\n",
    "        if \"candidates\" not in response or not response[\"candidates\"]:\n",
    "            return []\n",
    "\n",
    "        response_text = response[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "\n",
    "        # Try to parse the JSON directly from the response text\n",
    "        try:\n",
    "            # Clean up the response text to ensure it's valid JSON\n",
    "            # Find where JSON starts (typically at the beginning or after code blocks)\n",
    "            json_start = response_text.find(\"{\")\n",
    "            if json_start >= 0:\n",
    "                response_text = response_text[json_start:]\n",
    "\n",
    "            # Find where JSON ends (typically at the end or before additional commentary)\n",
    "            json_end = response_text.rfind(\"}\")\n",
    "            if json_end >= 0:\n",
    "                response_text = response_text[:json_end+1]\n",
    "\n",
    "            # Parse the JSON\n",
    "            result = json.loads(response_text)\n",
    "\n",
    "            # Extract annotations\n",
    "            if \"annotations\" in result:\n",
    "                entities = []\n",
    "                for annotation in result[\"annotations\"]:\n",
    "                    # Convert the new format to our internal format\n",
    "                    if all(k in annotation for k in [\"mention\", \"type\", \"begin\", \"end\"]):\n",
    "                        entities.append({\n",
    "                            \"mention\": annotation[\"mention\"],\n",
    "                            \"type\": annotation[\"type\"],\n",
    "                            \"begin\": annotation[\"begin\"],\n",
    "                            \"end\": annotation[\"end\"] - 1  # Convert exclusive end to inclusive end\n",
    "                        })\n",
    "                return entities\n",
    "            return []\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # If direct parsing fails, use regex to extract JSON\n",
    "            json_pattern = r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}'\n",
    "            match = re.search(json_pattern, response_text)\n",
    "            if match:\n",
    "                try:\n",
    "                    result = json.loads(match.group(0))\n",
    "                    if \"annotations\" in result:\n",
    "                        entities = []\n",
    "                        for annotation in result[\"annotations\"]:\n",
    "                            if all(k in annotation for k in [\"mention\", \"type\", \"begin\", \"end\"]):\n",
    "                                entities.append({\n",
    "                                    \"mention\": annotation[\"mention\"],\n",
    "                                    \"type\": annotation[\"type\"],\n",
    "                                    \"begin\": annotation[\"begin\"],\n",
    "                                    \"end\": annotation[\"end\"] - 1  # Convert exclusive end to inclusive end\n",
    "                                })\n",
    "                        return entities\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # If all else fails, look for individual annotations\n",
    "            entities = []\n",
    "            pattern = r'\"mention\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"type\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"begin\"\\s*:\\s*(\\d+)\\s*,\\s*\"end\"\\s*:\\s*(\\d+)'\n",
    "            matches = re.finditer(pattern, response_text)\n",
    "\n",
    "            for match in matches:\n",
    "                entities.append({\n",
    "                    \"mention\": match.group(1),\n",
    "                    \"type\": match.group(2),\n",
    "                    \"begin\": int(match.group(3)),\n",
    "                    \"end\": int(match.group(4)) - 1  # Convert exclusive end to inclusive end\n",
    "                })\n",
    "\n",
    "            return entities\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting entities: {e}\")\n",
    "        return []\n",
    "\n",
    "def align_token_to_char_positions(tokens: List[str], text: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Align token positions to character positions in the original text.\n",
    "    Returns a list of (start, end) character positions for each token.\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    start = 0\n",
    "    for token in tokens:\n",
    "        # Find the token in the text, starting from the current position\n",
    "        token_start = text.find(token, start)\n",
    "        if token_start == -1:\n",
    "            # If exact token not found, try with flexible whitespace\n",
    "            token_clean = token.strip()\n",
    "            token_start = text.find(token_clean, start)\n",
    "            if token_start == -1:\n",
    "                # If still not found, use approximate matching\n",
    "                token_start = start\n",
    "\n",
    "        token_end = token_start + len(token) - 1\n",
    "        positions.append((token_start, token_end))\n",
    "        start = token_end + 1\n",
    "\n",
    "    return positions\n",
    "\n",
    "def convert_char_entities_to_token_labels(entities: List[Dict[str, Any]],\n",
    "                                         token_positions: List[Tuple[int, int]],\n",
    "                                         num_tokens: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert character-based entity mentions to token-level BIO labels.\n",
    "    \"\"\"\n",
    "    labels = ['O'] * num_tokens\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_begin = entity['begin']\n",
    "        entity_end = entity['end']\n",
    "        entity_type = entity['type']\n",
    "\n",
    "        # Find the tokens that overlap with this entity\n",
    "        for i, (token_start, token_end) in enumerate(token_positions):\n",
    "            # Check if token overlaps with the entity\n",
    "            if (token_start <= entity_end and token_end >= entity_begin):\n",
    "                # Determine if this is the beginning or inside of the entity\n",
    "                if i == 0 or token_positions[i-1][1] < entity_begin:\n",
    "                    labels[i] = f'B-{entity_type}'\n",
    "                else:\n",
    "                    labels[i] = f'I-{entity_type}'\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(examples: List[Dict[str, Any]], system_prompt_path: str, batch_size: int = 1,\n",
    "           output_dir: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate Gemini on the given examples and return metrics.\n",
    "    If output_dir is provided, save intermediate results.\n",
    "    \"\"\"\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    # For intermediate results - organized by example\n",
    "    gold_entities_by_example = []\n",
    "    predicted_entities_by_example = []\n",
    "\n",
    "    # Read the system prompt from file\n",
    "    with open(system_prompt_path, 'r', encoding='utf-8') as f:\n",
    "      system_prompt = f.read()\n",
    "\n",
    "    for i in tqdm(range(0, len(examples), batch_size)):\n",
    "        batch = examples[i:i+batch_size]\n",
    "\n",
    "        for example_idx, example in enumerate(batch):\n",
    "            # Get the gold entities and text\n",
    "            tokens = example['tokens']\n",
    "            gold_labels = example['labels']\n",
    "            text = example['text']\n",
    "            entity_types = example['entity_types']\n",
    "\n",
    "            # Convert gold BIO tags to entity spans\n",
    "            gold_entities = convert_bio_to_entities(tokens, gold_labels, entity_types)\n",
    "\n",
    "            # Save gold entities for this example\n",
    "            gold_entities_by_example.append({\n",
    "                'example_id': i + example_idx,\n",
    "                'source_text': text,\n",
    "                'entities': gold_entities\n",
    "            })\n",
    "\n",
    "            # Call Gemini API with just the text (system prompt handles the rest)\n",
    "            # prompt  = create_gemini_prompt(text)\n",
    "            response = call_gemini_api(text, system_prompt)\n",
    "\n",
    "            # Extract predicted entities\n",
    "            pred_entities = extract_entities_from_response(response, text)\n",
    "\n",
    "            # Save predicted entities with the raw response\n",
    "            predicted_entities_by_example.append({\n",
    "                'example_id': i + example_idx,\n",
    "                'source_text': text,\n",
    "                'entities': pred_entities,\n",
    "                'raw_api_response': response.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')\n",
    "            })\n",
    "\n",
    "            # Map character positions to token positions\n",
    "            token_positions = align_token_to_char_positions(tokens, text)\n",
    "\n",
    "            # Convert predicted entities to BIO labels\n",
    "            pred_labels = convert_char_entities_to_token_labels(\n",
    "                pred_entities, token_positions, len(tokens))\n",
    "\n",
    "            all_true_labels.append(gold_labels)\n",
    "            all_pred_labels.append(pred_labels)\n",
    "\n",
    "            # Avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    # Save intermediate results if output directory is provided\n",
    "    if output_dir:\n",
    "        gold_entities_path = os.path.join(output_dir, \"gold_entities.json\")\n",
    "        pred_entities_path = os.path.join(output_dir, \"predicted_entities.json\")\n",
    "\n",
    "        save_intermediate_results(gold_entities_by_example, gold_entities_path)\n",
    "        save_intermediate_results(predicted_entities_by_example, pred_entities_path)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(all_true_labels, all_pred_labels)\n",
    "    recall = recall_score(all_true_labels, all_pred_labels)\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "    report = classification_report(all_true_labels, all_pred_labels, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_intermediate_results(data: List[Dict[str, Any]], file_path: str):\n",
    "    \"\"\"\n",
    "    Save intermediate results to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Saved intermediate results to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "sys.argv = [\n",
    "    \"Gemini_2.0_Biomedical_NER_Evaluation_Script.py\",\n",
    "    \"--data_dir\", \"./NERdata_gemini/NCBI-disease/\",\n",
    "    \"--output_dir\", \"./NERdata_gemini/output_eval\",\n",
    "    \"--batch_size\", \"5\",\n",
    "    \"--max_seq_length\", \"128\",\n",
    "    \"--system_prompt\", \"./NERdata_gemini/prompts/prompt_1.txt\"\n",
    "]\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "# Load the evaluation data\n",
    "data_files = [f for f in os.listdir(args.data_dir) if f.endswith('.txt')]\n",
    "\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_disease_n10.txt...\n",
      "Loaded 10 examples from test_disease_n10.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to ./NERdata_gemini/output_eval/test_disease_n10/gold_entities.json\n",
      "Saved intermediate results to ./NERdata_gemini/output_eval/test_disease_n10/predicted_entities.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bio_ner-3.10/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/bio_ner-3.10/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int64 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m file_results_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_results_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/bio_ner-3.10/lib/python3.10/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/opt/conda/envs/bio_ner-3.10/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/bio_ner-3.10/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/bio_ner-3.10/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/bio_ner-3.10/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/bio_ner-3.10/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/bio_ner-3.10/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "for file_name in data_files:\n",
    "    print(f\"Processing {file_name}...\")\n",
    "    file_path = os.path.join(args.data_dir, file_name)\n",
    "\n",
    "    # Read the data\n",
    "    examples = read_bio_data(file_path)\n",
    "    print(f\"Loaded {len(examples)} examples from {file_name}\")\n",
    "\n",
    "    # Evaluate and save intermediate results\n",
    "    file_output_dir = os.path.join(args.output_dir, os.path.splitext(file_name)[0])\n",
    "    os.makedirs(file_output_dir, exist_ok=True)\n",
    "\n",
    "    metrics = evaluate(\n",
    "        examples,\n",
    "        args.system_prompt,\n",
    "        args.batch_size,\n",
    "        output_dir=file_output_dir\n",
    "    )\n",
    "\n",
    "    # Save results for this file\n",
    "    all_results[file_name] = metrics\n",
    "\n",
    "    # Save detailed results for this file\n",
    "    file_results_path = os.path.join(args.output_dir, f\"{file_name}_results.json\")\n",
    "    with open(file_results_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"Results for {file_name}:\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1: {metrics['f1']:.4f}\")\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "for file_name in data_files:\n",
    "    print(f\"Processing {file_name}...\")\n",
    "    file_path = os.path.join(args.data_dir, file_name)\n",
    "\n",
    "    # Read the data\n",
    "    examples = read_bio_data(file_path)\n",
    "    print(f\"Loaded {len(examples)} examples from {file_name}\")\n",
    "\n",
    "    # Evaluate and save intermediate results\n",
    "    file_output_dir = os.path.join(args.output_dir, os.path.splitext(file_name)[0])\n",
    "    os.makedirs(file_output_dir, exist_ok=True)\n",
    "\n",
    "    metrics = evaluate(\n",
    "        examples,\n",
    "        entity_types,\n",
    "        args.batch_size,\n",
    "        output_dir=file_output_dir\n",
    "    )\n",
    "\n",
    "    # Save results for this file\n",
    "    all_results[file_name] = metrics\n",
    "\n",
    "    # Save detailed results for this file\n",
    "    file_results_path = os.path.join(args.output_dir, f\"{file_name}_results.json\")\n",
    "    with open(file_results_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"Results for {file_name}:\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1: {metrics['f1']:.4f}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "# Calculate and save aggregate results\n",
    "aggregate_precision = np.mean([res['precision'] for res in all_results.values()])\n",
    "aggregate_recall = np.mean([res['recall'] for res in all_results.values()])\n",
    "aggregate_f1 = np.mean([res['f1'] for res in all_results.values()])\n",
    "\n",
    "aggregate_results = {\n",
    "    'precision': float(aggregate_precision),\n",
    "    'recall': float(aggregate_recall),\n",
    "    'f1': float(aggregate_f1),\n",
    "    'file_results': all_results\n",
    "}\n",
    "\n",
    "# Save aggregate results\n",
    "aggregate_results_path = os.path.join(args.output_dir, \"aggregate_results.json\")\n",
    "with open(aggregate_results_path, 'w') as f:\n",
    "    json.dump(aggregate_results, f, indent=2)\n",
    "\n",
    "print(\"Aggregate Results:\")\n",
    "print(f\"Precision: {aggregate_precision:.4f}\")\n",
    "print(f\"Recall: {aggregate_recall:.4f}\")\n",
    "print(f\"F1: {aggregate_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ner-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
