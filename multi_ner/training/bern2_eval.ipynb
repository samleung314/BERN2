{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bern2-gpu/lib:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n"
     ]
    }
   ],
   "source": [
    "!echo $CONDA_PREFIX/lib:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bern2-gpu/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-06 22:52:46.656976: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-06 22:52:46.760159: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import subprocess\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "# import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "import wandb\n",
    "from utils_ner import *\n",
    "from modeling import *\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n",
    "    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n",
    "    # or just modify its tokenizer_config.json.\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
    "    )\n",
    "    labels: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    data_list: str = field(\n",
    "        default='', metadata={'help': 'List of entity name string'}\n",
    "    )\n",
    "    eval_data_list: str = field(\n",
    "        default='', metadata={'help': 'Entity name or Entity type for evaluation'}\n",
    "    )\n",
    "    eval_data_type: str = field(\n",
    "        default='', metadata={'help': 'Entity name or Entity type for evaluation'}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2025 22:56:03 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/06/2025 22:56:03 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./output, overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Mar06_22-56-03_instance-20240927-l4-gpu, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "  # See all possible arguments in src/transformers/training_args.py\n",
    "  # or by passing the --help flag to this script.\n",
    "  # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "\n",
    "MODEL_NAME = \"./bern2-ner\"\n",
    "OUTPUT_DIR = \"./output\"\n",
    "ENTITY= \"NCBI-disease\" # [\"NCBI-disease\", \"BC2GM\", \"BC4CHEMD\", \"JNLPBA-cl\",\n",
    "        # \"JNLPBA-ct\", \"JNLPBA-dna\", \"JNLPBA-rna\", \"linnaeus\"]\n",
    "BATCH_SIZE = 32\n",
    "SEED = 1\n",
    "\n",
    "# eval_data_types = \" \".join(ENTITY)\n",
    "# eval_data_lists = \" \".join(ENTITY)\n",
    "\n",
    "# Build the sys.argv list\n",
    "sys.argv = [\n",
    "    \"run_eval.py\",\n",
    "    \"--model_name_or_path\", MODEL_NAME,\n",
    "    \"--data_dir\", \"NERdata/\",\n",
    "    \"--labels\", \"NERdata/NCBI-disease/labels.txt\",\n",
    "    \"--output_dir\", OUTPUT_DIR,\n",
    "    \"--max_seq_length\", \"128\",\n",
    "    \"--per_device_eval_batch_size\", str(BATCH_SIZE),\n",
    "    \"--seed\", str(SEED),\n",
    "    \"--do_eval\",\n",
    "    \"--do_predict\",\n",
    "    \"--eval_data_type\", ENTITY, # eval_data_types,\n",
    "    \"--eval_data_list\", ENTITY # eval_data_lists,\n",
    "]\n",
    "\n",
    "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "    # If we pass only one argument to the script and it's the path to a json file,\n",
    "    # let's parse it to get our arguments.\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "else:\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2025 22:57:04 - INFO - utils_ner -   Creating features from dataset file at NERdata/\n",
      "0it [00:00, ?it/s]03/06/2025 22:57:04 - INFO - utils_ner -   Writing example 0 of 923\n",
      "03/06/2025 22:57:04 - INFO - utils_ner -   *** Example ***\n",
      "03/06/2025 22:57:04 - INFO - utils_ner -   guid: devel-1\n",
      "03/06/2025 22:57:04 - INFO - utils_ner -   tokens: <s> BRCA 1 is secreted and ex hibit s proper ties of a gran in . </s>\n",
      "03/06/2025 22:57:04 - INFO - utils_ner -   input_ids: 0 39123 20 284 49563 538 877 887 86 45209 1282 1560 68 21194 262 17 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "03/06/2025 22:57:04 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/06/2025 22:57:04 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/06/2025 22:57:04 - INFO - utils_ner -   label_ids: -100 2 -100 2 2 2 2 -100 -100 2 -100 2 2 2 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "03/06/2025 22:57:04 - INFO - utils_ner -   entity_type_ids: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "923it [00:02, 322.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Prepare CONLL-2003 task\n",
    "entity_name = data_args.data_dir[0].split('/')[-1]\n",
    "if entity_name in [\"CoNLL2003NER\", \"OntoNotes5.0\", \"WNUT2017\"]:\n",
    "    labels = get_labels(data_args.labels)\n",
    "else:\n",
    "    labels = get_bio_labels(data_args.labels)\n",
    "label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast,\n",
    ")\n",
    "\n",
    "# logger.info('MODEL NAME: ', model_args.model_name_or_path)\n",
    "# logger.info('CONFIG: ', config)\n",
    "model = RoBERTaMultiNER2.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=False,  # bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    num_labels=num_labels,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "\n",
    "# Get datasets\n",
    "train_dataset = (\n",
    "    NerDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.train,\n",
    "        eval_data_list=data_args.eval_data_list,\n",
    "    )\n",
    "    if training_args.do_train\n",
    "    else None\n",
    ")\n",
    "eval_dataset = (\n",
    "    NerDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.dev,\n",
    "        eval_data_list=data_args.eval_data_list,\n",
    "    )\n",
    "    if training_args.do_eval\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "    # Extract the logits array from predictions\n",
    "    logits = predictions[0]\n",
    "    preds = np.argmax(logits, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    # trainer.save_model()\n",
    "    # For convenience, we also re-save the tokenizer to the same directory,\n",
    "    # so that you can share your model easily on huggingface.co/models =)\n",
    "    if trainer.is_world_process_zero():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    result = trainer.evaluate()\n",
    "\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "    if trainer.is_world_process_zero():\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key, value in result.items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        results.update(result)\n",
    "\n",
    "# Predict\n",
    "if training_args.do_predict:\n",
    "    test_dataset = NerDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.test,\n",
    "        eval_data_list=data_args.eval_data_list,\n",
    "    )\n",
    "\n",
    "    predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "    preds_list, _ = align_predictions(predictions, label_ids)\n",
    "\n",
    "    # Save predictions\n",
    "    output_test_results_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n",
    "    if trainer.is_world_process_zero():\n",
    "        with open(output_test_results_file, \"w\") as writer:\n",
    "            logger.info(\"***** Test results *****\")\n",
    "            for key, value in metrics.items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bern2-gpu",
   "language": "python",
   "name": "bern2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
